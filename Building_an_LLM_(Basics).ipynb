{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9ZKF4xX4aQTVRaIUaA+yp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/John-Spenceley/AI-Basics/blob/main/Building_an_LLM_(Basics).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA2dDbk3_gc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/github/John-Spenceley/AI-Basics/blob/main/Building_an_LLM_(Basics).ipynb)\n"
      ],
      "metadata": {
        "id": "S_ZlmcN4gLqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**How to Build an LLM Playground (In Detail)**"
      ],
      "metadata": {
        "id": "WHsGIvgQ_oQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: Create a hands-on, code-light path to understanding how Large Language Models work—so non-coders can use, evaluate, and eventually tailor an LLM without getting bogged down in engineering details."
      ],
      "metadata": {
        "id": "G2b-9XMOAC5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Who This Is For:**\n",
        "\n",
        "\n",
        "\n",
        "*   Non-coders / beginners who have a rudimentary understanding of Python\n",
        "*   Product, research, or ops folks who want to reason about LLM behavior\n",
        "*   Makers planning a simple, focused LLM for real users\n",
        "\n",
        "**Required Tools:**\n",
        "\n",
        "*   Google Colab (https://colab.research.google.com/)\n",
        "*   Optional local run with Jupyter/VS Code\n"
      ],
      "metadata": {
        "id": "eKxAtOeFAxNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Objectives**\n",
        "\n",
        "1. **Tokenization** — Turning Text into Tokens\n",
        "\n",
        "*   Understand how raw text is split into a sequence of discrete tokens (the building blocks of LLMs).\n",
        "*   Visualize how punctuation, emojis, and word fragments are represented.\n",
        "*   Learn why tokenization affects both cost and creativity in text generation.\n",
        "\n",
        "\n",
        "2. **Inspecting GPT-2 & Transformer Architecture**\n",
        "\n",
        "*   Explore the core building blocks of LLMs: embeddings, self-attention, and layers.\n",
        "*   See how GPT-2 represents the broader class of Transformer-based models that power today’s AI systems.\n",
        "*   Understand at a conceptual level how models “predict the next token” to form language.\n",
        "\n",
        "\n",
        "\n",
        "3. **Loading Pre-Trained LLMs (Using Hugging Face)**\n",
        "\n",
        "*   Learn how to load existing pre-trained models in one line of code using the Hugging Face library.\n",
        "*   No training required—just loading, prompting, and observing.\n",
        "*   Gain confidence in exploring different model types (GPT-2, Qwen, etc.).\n",
        "\n",
        "4. **Decoding Strategies — How Models Generate Text**\n",
        "\n",
        "*   Experiment with decoding parameters: temperature, top-k, and top-p.\n",
        "*  Understand how these affect creativity, coherence, and factuality.\n",
        "\n",
        "*   Compare deterministic (greedy) vs. probabilistic sampling.\n",
        "\n",
        "5. **Completion vs. Instruction Fine-Tuned Models**\n",
        "\n",
        "*   Learn the difference between completion models (predict the next word) and instruction-tuned models (follow directions).\n",
        "\n",
        "*   Understand why instruction tuning makes models like ChatGPT easier for everyday users—especially non-coders.\n",
        "*   Practice prompting both types and see how their behaviors differ."
      ],
      "metadata": {
        "id": "qECtTcE3DS1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By the End, You’ll Be Able To:**\n",
        "\n",
        "*   Explain what tokenization means and why it matters.\n",
        "*   Describe the basic structure of a Transformer model.\n",
        "*   Load and interact with pre-trained models confidently.\n",
        "*   Adjust decoding strategies to control style and randomness.\n",
        "*   Differentiate between completion and instruction-tuned LLMs—knowing which is better for non-coder projects.\n"
      ],
      "metadata": {
        "id": "DVEKzcljGSm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setup Cell — Import and Version Check**\n",
        "\n",
        "1. **Purpose**\n",
        "   * Ensure that all required LLM libraries are installed and correctly loaded in your Colab environment.\n",
        "   * Confirm compatibility between the deep-learning framework (PyTorch), the model library (Transformers), and the tokenizer library (TikToken).\n",
        "   * This verification helps prevent runtime errors caused by version mismatches.\n",
        "\n",
        "2. **Libraries Used**\n",
        "   * **torch** — Core deep learning framework (handles tensors, GPU computation, and neural network training).\n",
        "   * **transformers** — Hugging Face library providing access to pre-trained LLMs like GPT-2, BERT, and Qwen.\n",
        "   * **tiktoken** — OpenAI’s fast tokenizer that converts text into tokens (numbers) and back.\n",
        "\n",
        "3. **What Happens**\n",
        "   * The libraries are imported.\n",
        "   * The versions of `torch` and `transformers` are printed to verify installation.\n",
        "   * This acts as a quick diagnostic step before loading or running any model.\n"
      ],
      "metadata": {
        "id": "QiG7GZQIQMD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, transformers, tiktoken\n",
        "print(\"torch\", torch.__version__, \"| transformers\", transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rql5hlPuOhqi",
        "outputId": "7386dccc-0844-4c6a-b0db-4054269b9c95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch 2.8.0+cu126 | transformers 4.57.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization — Turning Text into Tokens\n",
        "\n",
        "A neural network can’t digest raw text — it needs numbers.  \n",
        "Tokenization is the process of converting text into integer IDs that a model can understand.\n",
        "\n",
        "In this section, you'll learn how tokenization is implemented in practice.\n",
        "\n",
        "### Tokenization Methods\n",
        "Tokenization methods generally fall into three main categories:\n",
        "\n",
        "1. **Word-level tokenization** — Split text by spaces; each word becomes a token.  \n",
        "2. **Character-level tokenization** — Each character (letter, punctuation, emoji) becomes a token.  \n",
        "3. **Subword-level tokenization** — Breaks words into smaller pieces for efficiency and flexibility (used by GPT-2 and most modern LLMs).\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1 – Word-Level Tokenization\n",
        "Split text on whitespace and store each word as a token."
      ],
      "metadata": {
        "id": "dkspO9JfQLTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tiny corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Tokenization converts text to numbers\",\n",
        "    \"Large language models predict the next token\"\n",
        "]\n",
        "\n",
        "# 2. Build the vocabulary\n",
        "PAD, UNK = \"[PAD]\", \"[UNK]\"\n",
        "words = set()\n",
        "for doc in corpus:\n",
        "    words.update(doc.lower().split())\n",
        "\n",
        "vocab = [PAD, UNK] + sorted(words)\n",
        "word2id = {w: i for i, w in enumerate(vocab)}\n",
        "id2word = {i: w for w, i in word2id.items()}\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)} words\")\n",
        "print(\"First 15 vocab entries:\", vocab[:15])\n",
        "\n",
        "# 3. Encode / decode functions\n",
        "def encode(text):\n",
        "    return [word2id.get(w, word2id[UNK]) for w in text.lower().split()]\n",
        "\n",
        "def decode(ids):\n",
        "    return \" \".join(id2word[i] for i in ids if i != word2id[PAD])\n",
        "\n",
        "# 4. Demo\n",
        "sample = \"The brown unicorn jumps\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Decoded    :\", recovered)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykmmUZ3FRRWu",
        "outputId": "4c94a500-dd13-4535-db6a-62e74f8143fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 21 words\n",
            "First 15 vocab entries: ['[PAD]', '[UNK]', 'brown', 'converts', 'dog', 'fox', 'jumps', 'language', 'large', 'lazy', 'models', 'next', 'numbers', 'over', 'predict']\n",
            "\n",
            "Input text : The brown unicorn jumps\n",
            "Token IDs  : [17, 2, 1, 6]\n",
            "Decoded    : the brown [UNK] jumps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Output**\n",
        "\n",
        "1. **Vocabulary size: 21 words**  \n",
        "   The model found 21 unique tokens (words) in the sample text collection, plus two special tokens — `[PAD]` and `[UNK]`.  \n",
        "   These form the *vocabulary*, which is the list of all known words.\n",
        "\n",
        "2. **First 15 vocab entries:**  \n",
        "   `['[PAD]', '[UNK]', 'brown', 'converts', 'dog', 'fox', 'jumps', 'language', 'large', 'lazy', 'models', 'next', 'numbers', 'over', 'predict']`  \n",
        "   This shows the first 15 tokens in alphabetical order.  \n",
        "   *`[PAD]`* is used for padding shorter sequences, and *`[UNK]`* represents unknown words.\n",
        "\n",
        "3. **Input text : The brown unicorn jumps**  \n",
        "   This is the example sentence being tokenized.\n",
        "\n",
        "4. **Token IDs  : [17, 2, 1, 6]**  \n",
        "   Each number corresponds to the position of a word in the vocabulary:  \n",
        "   - `17` = \"the\"  \n",
        "   - `2`  = \"brown\"  \n",
        "   - `1`  = `[UNK]` (unknown word, because “unicorn” isn’t in the vocabulary)  \n",
        "   - `6`  = \"jumps\"\n",
        "\n",
        "5. **Decoded : the brown [UNK] jumps**  \n",
        "   The tokens are converted back into words.  \n",
        "   “Unicorn” was replaced with `[UNK]` because it wasn’t in the vocabulary.\n",
        "\n",
        "**In summary:**  \n",
        "This output demonstrates how tokenization turns words into numerical IDs, and how unknown words are handled when they don’t exist in the training vocabulary.\n"
      ],
      "metadata": {
        "id": "3zwbd87hT1Eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 – Character-Level Tokenization\n",
        "\n",
        "Every single character (including spaces, punctuation, and emojis) gets its own ID.  \n",
        "This guarantees zero out-of-vocabulary (OOV) issues but produces much longer sequences.\n",
        "\n",
        "Character-level tokenization is useful when dealing with small vocabularies or languages with many rare words, but it’s computationally heavier since each word is broken into multiple characters.\n"
      ],
      "metadata": {
        "id": "GVo-fGwOT3Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Build a fixed vocabulary\n",
        "import string\n",
        "\n",
        "letters = list(string.ascii_lowercase + string.ascii_uppercase)  # a–z + A–Z\n",
        "special = [\"[PAD]\", \"[UNK]\"]  # padding + unknown\n",
        "vocab = special + letters\n",
        "\n",
        "char2id = {ch: idx for idx, ch in enumerate(vocab)}\n",
        "id2char = {idx: ch for ch, idx in char2id.items()}\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)} (52 letters + 2 specials)\")\n",
        "\n",
        "# 2. Encode / decode\n",
        "def encode(text):\n",
        "    \"\"\"Convert text → list of IDs (unknown chars → [UNK]).\"\"\"\n",
        "    unk_id = char2id[\"[UNK]\"]\n",
        "    return [char2id.get(ch, unk_id) for ch in text]\n",
        "\n",
        "def decode(ids):\n",
        "    \"\"\"Convert list of IDs back to characters.\"\"\"\n",
        "    return \"\".join(id2char[i] for i in ids if i != char2id[\"[PAD]\"])\n",
        "\n",
        "# 3. Demo\n",
        "sample = \"Hello\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Decoded    :\", recovered)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paOIKZ_RW5Eo",
        "outputId": "fb44c7d1-9995-45cd-c718-e26bcb392528"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 54 (52 letters + 2 specials)\n",
            "\n",
            "Input text : Hello\n",
            "Token IDs  : [35, 6, 13, 13, 16]\n",
            "Decoded    : Hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Output**\n",
        "\n",
        "1. **Vocabulary size: 54 (52 letters + 2 specials)**  \n",
        "   There are 26 lowercase and 26 uppercase letters, plus two special tokens:  \n",
        "   `[PAD]` for padding sequences, and `[UNK]` for unknown characters.\n",
        "\n",
        "2. **Input text : Hello**  \n",
        "   The string you want to tokenize.\n",
        "\n",
        "3. **Token IDs :**  \n",
        "   Each character in “Hello” is converted to its numeric ID using the `char2id` dictionary.  \n",
        "   For example, `H`, `e`, `l`, and `o` each have their own assigned number.\n",
        "\n",
        "4. **Decoded : Hello**  \n",
        "   The token IDs are converted back to characters using the `id2char` mapping, reconstructing the original text.\n",
        "\n",
        "**In summary:**  \n",
        "This demonstrates how character-level tokenization works.  \n",
        "Each character (including case differences) becomes its own token, eliminating OOV errors but resulting in longer token sequences.\n"
      ],
      "metadata": {
        "id": "NVGslbiOW-Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 – Subword-Level Tokenization\n",
        "\n",
        "Subword methods such as **Byte-Pair Encoding (BPE)**, **WordPiece**, and **SentencePiece** learn the most common character combinations and group them into tokens.  \n",
        "\n",
        "For example, the word *unbelievable* might be split into three tokens:  \n",
        "`[\"un\", \"believ\", \"able\"]`\n",
        "\n",
        "This approach strikes a balance between word-level and character-level methods, solving their main limitations — it handles unknown words efficiently while keeping sequence lengths manageable.\n",
        "\n",
        "---\n",
        "\n",
        "### How BPE Works\n",
        "\n",
        "1. Start with individual characters (bytes) — each is its own token.  \n",
        "2. Count all adjacent pairs of tokens in a large corpus.  \n",
        "3. Merge the most frequent pair into a new token.  \n",
        "4. Repeat steps 2–3 until you reach the target vocabulary size (e.g., 50 000).  \n",
        "\n",
        "Let’s see **BPE** in practice using GPT-2’s pretrained tokenizer."
      ],
      "metadata": {
        "id": "YVqer53oXBbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load a pretrained BPE tokenizer (GPT-2 uses BPE)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "bpe_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(\"Vocab size:\", bpe_tok.vocab_size)\n",
        "print(\"Special tokens:\", bpe_tok.all_special_tokens)\n",
        "\n",
        "# 2. Encode / decode\n",
        "def encode(text):\n",
        "    return bpe_tok.encode(text)\n",
        "\n",
        "def decode(ids):\n",
        "    return bpe_tok.decode(ids)\n",
        "\n",
        "# 3. Demo\n",
        "sample = \"Unbelievable tokenization powers! 🚀\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Tokens     :\", bpe_tok.convert_ids_to_tokens(ids))\n",
        "print(\"Decoded    :\", recovered)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5xL4fq_XM4v",
        "outputId": "c5624056-2540-4e8f-f9ce-fa8d06c813f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 50257\n",
            "Special tokens: ['<|endoftext|>']\n",
            "\n",
            "Input text : Unbelievable tokenization powers! 🚀\n",
            "Token IDs  : [3118, 6667, 11203, 540, 11241, 1634, 5635, 0, 12520, 248, 222]\n",
            "Tokens     : ['Un', 'bel', 'iev', 'able', 'Ġtoken', 'ization', 'Ġpowers', '!', 'ĠðŁ', 'ļ', 'Ģ']\n",
            "Decoded    : Unbelievable tokenization powers! 🚀\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Output**\n",
        "\n",
        "1. **Vocab size**  \n",
        "   Shows how many unique subword tokens exist in GPT-2’s vocabulary (about 50 000).\n",
        "\n",
        "2. **Special tokens**  \n",
        "   Lists tokens like `` or `` that mark text boundaries or padding.\n",
        "\n",
        "3. **Token IDs**  \n",
        "   Each subword in the input is converted into its corresponding numerical ID.\n",
        "\n",
        "4. **Tokens**  \n",
        "   Displays the actual subword pieces created by BPE — for example:  \n",
        "   `[\"Un\", \"believ\", \"able\", \" token\", \"ization\", \" powers\", \"!\", \"Ġ🚀\"]`\n",
        "\n",
        "5. **Decoded text**  \n",
        "   Converts the token IDs back into readable text, confirming that the tokenizer can perfectly reconstruct the original input.\n",
        "\n",
        "**In summary:**  \n",
        "Subword-level tokenization allows language models to handle both familiar and unseen words efficiently by combining the flexibility of character-level methods with the compactness of word-level ones.\n"
      ],
      "metadata": {
        "id": "RIDya_toXRyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 – TikToken\n",
        "\n",
        "`tiktoken` is a production-ready, high-speed tokenization library used by OpenAI models.  \n",
        "It’s optimized for performance and supports the same tokenization rules used in GPT-3, GPT-3.5, and GPT-4.\n",
        "\n",
        "In this section, we’ll compare the **older GPT-2 encoding** (`gpt2`) with the **newer GPT-4 encoding** (`cl100k_base`).\n",
        "\n",
        "The newer encoding supports a much larger vocabulary and handles emojis, punctuation, and multilingual text more efficiently.\n"
      ],
      "metadata": {
        "id": "VumSyhfSYTr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare GPT-2 and GPT-4 encodings using TikToken\n",
        "import tiktoken\n",
        "\n",
        "encodings = [\n",
        "    (\"gpt2\", tiktoken.get_encoding(\"gpt2\")),\n",
        "    (\"cl100k_base\", tiktoken.get_encoding(\"cl100k_base\")),\n",
        "]\n",
        "\n",
        "sentence = \"The 🌟 star-player scored 40 points!\"\n",
        "\n",
        "for name, enc in encodings:\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Vocabulary size:\", enc.n_vocab)\n",
        "\n",
        "    # Encode the sample sentence\n",
        "    ids = enc.encode(sentence)\n",
        "    tokens = [enc.decode([i]) for i in ids]\n",
        "    print(f\"Sentence splits into {len(ids)} tokens:\")\n",
        "    print(list(zip(tokens, ids)))\n",
        "\n",
        "    # Show a few arbitrary token→ID examples from the vocab\n",
        "    some_ids = [0, 1, 2, 198, 50256]\n",
        "    print(\"Sample tokens from the vocabulary:\")\n",
        "    print([(enc.decode([i]), i) for i in some_ids])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9UQV1idYWC9",
        "outputId": "fc18e933-a3f8-4e4c-d67e-25f00c21eeef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== gpt2 ===\n",
            "Vocabulary size: 50257\n",
            "Sentence splits into 11 tokens:\n",
            "[('The', 464), (' �', 12520), ('�', 234), ('�', 253), (' star', 3491), ('-', 12), ('player', 7829), (' scored', 7781), (' 40', 2319), (' points', 2173), ('!', 0)]\n",
            "Sample tokens from the vocabulary:\n",
            "[('!', 0), ('\"', 1), ('#', 2), ('\\n', 198), ('<|endoftext|>', 50256)]\n",
            "\n",
            "=== cl100k_base ===\n",
            "Vocabulary size: 100277\n",
            "Sentence splits into 11 tokens:\n",
            "[('The', 791), (' �', 11410), ('�', 234), ('�', 253), (' star', 6917), ('-player', 43467), (' scored', 16957), (' ', 220), ('40', 1272), (' points', 3585), ('!', 0)]\n",
            "Sample tokens from the vocabulary:\n",
            "[('!', 0), ('\"', 1), ('#', 2), ('\\n', 198), ('parable', 50256)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Output**\n",
        "\n",
        "1. **Vocabulary size**  \n",
        "   Displays how many tokens exist in each encoding.  \n",
        "   - `gpt2` uses around 50,000 tokens.  \n",
        "   - `cl100k_base` (used by GPT-4) supports over 100,000 tokens.\n",
        "\n",
        "2. **Sentence splits into ... tokens**  \n",
        "   Shows how the same sentence is divided differently depending on the encoding.  \n",
        "   GPT-4’s `cl100k_base` tends to use fewer tokens for the same text because it has a richer vocabulary.\n",
        "\n",
        "3. **Token–ID pairs**  \n",
        "   Each token (word, subword, or emoji) is paired with its corresponding integer ID — how models see text internally.\n",
        "\n",
        "4. **Sample tokens from the vocabulary**  \n",
        "   Prints a few tokens and their IDs to illustrate how special tokens or punctuation are represented.\n",
        "\n",
        "**In summary:**  \n",
        "`tiktoken` provides the exact tokenizer used by OpenAI models, allowing you to measure, visualize, and understand how text is split into tokens.  \n",
        "It’s especially useful when estimating token costs or preparing text for OpenAI API inputs.\n"
      ],
      "metadata": {
        "id": "CnA322fAYYs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization 1.5 – Key Takeaways\n",
        "\n",
        "**Word-level:**  \n",
        "Simple to implement but brittle — struggles with *out-of-vocabulary (OOV)* words that were not seen during training.\n",
        "\n",
        "**Character-level:**  \n",
        "Handles every possible input, but produces long token sequences and is less efficient for training large models.\n",
        "\n",
        "**Subword-level (BPE / Byte-Level BPE):**  \n",
        "Strikes a balance between the two — compact, efficient, and capable of representing new words through smaller subword units.  \n",
        "Used by most modern language models (e.g., GPT-2, GPT-3, GPT-4, BERT).\n",
        "\n",
        "**TikToken:**  \n",
        "Demonstrates how production-grade models tokenize using optimized, pre-trained subword vocabularies.  \n",
        "It provides the same fast and memory-efficient tokenization used inside OpenAI’s GPT models.\n"
      ],
      "metadata": {
        "id": "djhjgQhHYmE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 – What is a Language Model?\n",
        "\n",
        "At its core, a **language model (LM)** is a large mathematical function built from many neural-network layers.  \n",
        "Given a sequence of tokens [t₁, t₂, …, tₙ], it learns to output a probability for the next token tₙ₊₁.\n",
        "\n",
        "Each layer performs simple operations (matrix multiplication, attention, etc.).  \n",
        "Stacking hundreds of these layers allows the model to capture patterns and relationships in text.\n",
        "\n",
        "The final output is a vector of scores representing how likely each possible next token is.  \n",
        "You can think of the entire network as one enormous equation whose parameters were tuned during training to minimize prediction error.\n"
      ],
      "metadata": {
        "id": "HQE9bljwY4rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 – A Single Linear Layer\n",
        "Before exploring the Transformer, let’s start with the simplest building block.\n",
        "\n",
        "A **Linear layer** performs the operation *y = Wx + b*  \n",
        "where  \n",
        "- **x** is the input vector  \n",
        "- **W** is the learned weight matrix  \n",
        "- **b** is the learned bias vector  \n",
        "\n",
        "Chaining many such linear layers (with nonlinear activations in between) gives neural networks their expressive power.\n"
      ],
      "metadata": {
        "id": "c73QHdhBZF3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple Linear layer manually\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.matmul(x, self.weight.t()) + self.bias\n",
        "\n",
        "lin = Linear(3, 2)\n",
        "x = torch.tensor([1.0, -1.0, 0.5])\n",
        "print(\"Input :\", x)\n",
        "print(\"Weights:\", lin.weight)\n",
        "print(\"Bias   :\", lin.bias)\n",
        "print(\"Output :\", lin(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK0mJIO0ZH9e",
        "outputId": "75c97cf5-61f0-4b7a-9b3d-750df5976ed4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : tensor([ 1.0000, -1.0000,  0.5000])\n",
            "Weights: Parameter containing:\n",
            "tensor([[ 0.2942,  1.2544, -1.2355],\n",
            "        [ 0.6460, -0.2439, -0.0587]], requires_grad=True)\n",
            "Bias   : Parameter containing:\n",
            "tensor([ 1.5114, -0.5846], requires_grad=True)\n",
            "Output : tensor([-0.0665,  0.2759], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Same operation using PyTorch’s built-in Linear layer\n",
        "lin = nn.Linear(3, 2)\n",
        "x = torch.tensor([1.0, -1.0, 0.5])\n",
        "print(\"Input :\", x)\n",
        "print(\"Weights:\", lin.weight)\n",
        "print(\"Bias   :\", lin.bias)\n",
        "print(\"Output :\", lin(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-nBaK8FZNnR",
        "outputId": "2080d560-d965-4d37-d017-81c3ef2d2f92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : tensor([ 1.0000, -1.0000,  0.5000])\n",
            "Weights: Parameter containing:\n",
            "tensor([[-0.3131, -0.1700, -0.0695],\n",
            "        [ 0.5045,  0.1321, -0.1576]], requires_grad=True)\n",
            "Bias   : Parameter containing:\n",
            "tensor([-0.0692,  0.5535], requires_grad=True)\n",
            "Output : tensor([-0.2470,  0.8472], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**  \n",
        "The Linear layer multiplies the input vector by a learned weight matrix and adds a bias vector.  \n",
        "This transforms the input into a new representation — a basic step repeated thousands of times inside LLMs.\n"
      ],
      "metadata": {
        "id": "mapMWnRwZQEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 – A Transformer Layer\n",
        "Most LLMs are built as a stack of identical **Transformer blocks**, each containing two main parts:\n",
        "\n",
        "| Step | What it does | Where it lives in code |\n",
        "|:--|:--|:--|\n",
        "| Multi-Head Self-Attention | Each token looks at other tokens to decide what matters | `block.attn` |\n",
        "| Feed-Forward Network (MLP) | Re-mixes information token by token | `block.mlp` |\n",
        "\n",
        "Below we load the smallest public GPT-2 (124 M parameters), grab its first block, and inspect its modules.\n"
      ],
      "metadata": {
        "id": "lAd6UcqAZR1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Load GPT-2 (124 M parameters)\n",
        "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "block = gpt2.transformer.h[0]   # GPT-2 has 12 such layers\n",
        "\n",
        "for name, module in block.named_children():\n",
        "    print(f\"{name:7s} → {module.__class__.__name__}\")\n",
        "\n",
        "print(\"\\n=== First Transformer Block ===\")\n",
        "print(block, \"\\n\")\n",
        "\n",
        "# Run a tiny forward pass through one block\n",
        "seq_len = 8\n",
        "dummy_tokens = torch.randint(0, gpt2.config.vocab_size, (1, seq_len))\n",
        "\n",
        "with torch.no_grad():\n",
        "    hidden = (\n",
        "        gpt2.transformer.wte(dummy_tokens) +\n",
        "        gpt2.transformer.wpe(torch.arange(seq_len))\n",
        "    )\n",
        "    out = block(hidden, layer_past=None, use_cache=False)[0]\n",
        "\n",
        "print(\"\\nOutput shape :\", out.shape)   # (batch, seq_len, hidden_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsy_NEICZh5T",
        "outputId": "867f3889-e69f-4a72-8ac0-0f0c146083bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ln_1    → LayerNorm\n",
            "attn    → GPT2Attention\n",
            "ln_2    → LayerNorm\n",
            "mlp     → GPT2MLP\n",
            "\n",
            "=== First Transformer Block ===\n",
            "GPT2Block(\n",
            "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (attn): GPT2Attention(\n",
            "    (c_attn): Conv1D(nf=2304, nx=768)\n",
            "    (c_proj): Conv1D(nf=768, nx=768)\n",
            "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (mlp): GPT2MLP(\n",
            "    (c_fc): Conv1D(nf=3072, nx=768)\n",
            "    (c_proj): Conv1D(nf=768, nx=3072)\n",
            "    (act): NewGELUActivation()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ") \n",
            "\n",
            "\n",
            "Output shape : torch.Size([1, 8, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**  \n",
        "A Transformer block contains an **attention mechanism** and a **feed-forward network**.  \n",
        "Each token gathers information from others, enabling the model to capture long-range relationships.\n"
      ],
      "metadata": {
        "id": "xxrRz_AYZjRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 – Inside GPT-2\n",
        "GPT-2 is just many of those Transformer blocks arranged sequentially.  \n",
        "Let’s print the modules inside the Transformer stack.\n"
      ],
      "metadata": {
        "id": "UdxN7BMLZlmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in gpt2.transformer.named_children():\n",
        "    print(f\"{name:7s} → {module.__class__.__name__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaoPoOEgZq6E",
        "outputId": "0c0f98e3-04f9-496c-a038-889c9130b1ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wte     → Embedding\n",
            "wpe     → Embedding\n",
            "drop    → Dropout\n",
            "h       → ModuleList\n",
            "ln_f    → LayerNorm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Main Modules**\n",
        "\n",
        "| Step | What it does | Why it matters |\n",
        "|:--|:--|:--|\n",
        "| Token → Embedding | Converts token IDs into vectors | Gives the model numeric handles on words |\n",
        "| Positional Encoding | Adds information about word order | Order matters in language |\n",
        "| Multi-Head Self-Attention | Each token asks “which other tokens should I attend to?” | Captures context and relationships |\n",
        "| Feed-Forward Network | Two Linear layers with a non-linearity | Adds depth and pattern mixing |\n",
        "| LayerNorm & Residual | Stabilize training and help gradients flow | Keep deep models trainable |\n"
      ],
      "metadata": {
        "id": "dJ-8kdhZZwMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 – LLM Output\n",
        "Passing a token sequence through an LLM produces a tensor of **logits** with shape  \n",
        "*(batch_size, seq_len, vocab_size)*.  \n",
        "\n",
        "Applying `softmax` on the last dimension converts these logits into probabilities for each possible next token.\n"
      ],
      "metadata": {
        "id": "n2dF-MRVZzCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "# Load GPT-2 and tokenizer if needed\n",
        "try:\n",
        "    gpt2\n",
        "except NameError:\n",
        "    gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize input\n",
        "text = \"Hello my name\"\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids   # (1, seq_len)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = gpt2(input_ids).logits                         # (1, seq_len, vocab_size)\n",
        "\n",
        "print(\"Logits shape :\", logits.shape)\n",
        "\n",
        "# Predict next token\n",
        "probs = F.softmax(logits[0, -1], dim=-1)\n",
        "topk = torch.topk(probs, 5)\n",
        "\n",
        "print(\"\\nTop-5 predictions for the next token:\")\n",
        "for idx, p in zip(topk.indices.tolist(), topk.values.tolist()):\n",
        "    print(f\"{tokenizer.decode([idx]):>10s} — {p:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR2Dcb7xZ0Vb",
        "outputId": "cd8914fa-db1e-4eda-e8c4-5068abb6cba9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape : torch.Size([1, 3, 50257])\n",
            "\n",
            "Top-5 predictions for the next token:\n",
            "        is — 0.7773\n",
            "         , — 0.0373\n",
            "        's — 0.0332\n",
            "       was — 0.0127\n",
            "       and — 0.0076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**  \n",
        "The output logits represent scores for every token in the vocabulary.  \n",
        "After applying softmax, we get probabilities for the next token.  \n",
        "Printing the top-5 tokens shows which words the model believes are most likely to follow the input.\n"
      ],
      "metadata": {
        "id": "RQYYJwo7Z45n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 – Key Takeaway\n",
        "\n",
        "A language model is not mystical — it’s a large composition of simple, understandable layers trained to predict the next token in a sequence.\n",
        "\n",
        "By stacking Linear layers, attention mechanisms, and normalization steps at scale,  \n",
        "modern LLMs can capture grammar, context, and meaning from text data through pure pattern prediction.\n"
      ],
      "metadata": {
        "id": "eGVkyp3ZZ-hT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 – Generation\n",
        "\n",
        "Once a language model is trained to predict the probabilities of the next token, we can **generate text** from it.  \n",
        "This process is called **decoding** or **sampling**.\n",
        "\n",
        "At each step, the model outputs a probability distribution over the next token.  \n",
        "The decoding algorithm decides which token to pick next, then continues predicting subsequent tokens.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Decoding Strategies\n",
        "\n",
        "| Strategy | Description | Behavior |\n",
        "|:--|:--|:--|\n",
        "| **Greedy** | Always pick the single most probable next token | Deterministic, but can become repetitive |\n",
        "| **Top-k Sampling** | Randomly sample from the top-k most likely tokens | Adds variety while staying coherent |\n",
        "| **Nucleus (Top-p)** | Sample from the smallest set of tokens whose probabilities sum to *p* | Adapts dynamically to context |\n",
        "| **Beam Search** | Keeps multiple candidate sequences and expands the best ones | More structured, often used in translation |\n",
        "| **Temperature** | A “creativity knob”: higher values flatten the probability distribution | Lower = precise / Higher = diverse |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "M6GHEk5_aXF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 – Greedy Decoding\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "MODELS = {\"gpt2\": \"gpt2\"}\n",
        "tokenizers, models = {}, {}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load GPT-2\n",
        "for key, mid in MODELS.items():\n",
        "    tok = AutoTokenizer.from_pretrained(mid)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(mid).eval().to(device)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    mdl.config.pad_token_id = tok.pad_token_id\n",
        "    tokenizers[key], models[key] = tok, mdl\n",
        "    print(f\"Loaded {mid} as {key}\")\n",
        "\n",
        "# Generation function\n",
        "def generate(model_key, prompt, strategy=\"greedy\", max_new_tokens=100):\n",
        "    tok, mdl = tokenizers[model_key], models[model_key]\n",
        "    enc = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
        "    gen_args = dict(**enc, max_new_tokens=max_new_tokens, pad_token_id=tok.pad_token_id)\n",
        "\n",
        "    if strategy == \"greedy\":\n",
        "        gen_args[\"do_sample\"] = False\n",
        "    elif strategy == \"top_k\":\n",
        "        gen_args.update(dict(do_sample=True, top_k=50, temperature=0.9))\n",
        "    elif strategy == \"top_p\":\n",
        "        gen_args.update(dict(do_sample=True, top_p=0.9, temperature=0.9))\n",
        "\n",
        "    out = mdl.generate(**gen_args)\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# Demo: Greedy decoding\n",
        "tests = [\"Once upon a time\", \"What is 2+2?\", \"Suggest a party theme.\"]\n",
        "for prompt in tests:\n",
        "    print(f\"\\n== GPT-2 | Greedy ==\")\n",
        "    print(generate(\"gpt2\", prompt, \"greedy\", 80))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mBBYizLaZEK",
        "outputId": "c64365d5-2a5d-4347-d710-d290a05076cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded gpt2 as gpt2\n",
            "\n",
            "== GPT-2 | Greedy ==\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and\n",
            "\n",
            "== GPT-2 | Greedy ==\n",
            "What is 2+2?\n",
            "\n",
            "2+2 is the number of times you can use a spell to cast a spell.\n",
            "\n",
            "2+2 is the number of times you can use a spell to cast a spell.\n",
            "\n",
            "2+2 is the number of times you can use a spell to cast a spell.\n",
            "\n",
            "2+2 is the number of times you can use a spell to cast a spell.\n",
            "\n",
            "== GPT-2 | Greedy ==\n",
            "Suggest a party theme.\n",
            "\n",
            "The party theme is a simple, simple, and fun way to get your friends to join you.\n",
            "\n",
            "The party theme is a simple, simple, and fun way to get your friends to join you. The party theme is a simple, simple, and fun way to get your friends to join you. The party theme is a simple, simple, and fun way to get your friends\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "Greedy decoding always selects the highest-probability token at every step.  \n",
        "It’s efficient but can easily fall into repetition (e.g., “The cat is is is…”) and may miss more interesting continuations with slightly lower initial probability.\n"
      ],
      "metadata": {
        "id": "7U3J-SUsabm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 – Top-k and Top-p Sampling\n",
        "These methods introduce randomness for more natural and creative outputs.\n",
        "\n",
        "* **Top-k Sampling:** randomly sample from the top *k* most likely tokens.  \n",
        "* **Top-p (Nucleus) Sampling:** dynamically choose the smallest subset of tokens whose cumulative probability ≥ *p*.\n"
      ],
      "metadata": {
        "id": "Y8c1rzHraeZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Top-p Sampling\n",
        "tests = [\"Once upon a time\", \"What is 2+2?\", \"Suggest a party theme.\"]\n",
        "for prompt in tests:\n",
        "    print(f\"\\n== GPT-2 | Top-p ==\")\n",
        "    print(generate(\"gpt2\", prompt, \"top_p\", 40))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYMgcolKafnV",
        "outputId": "3adab4f5-f528-4d40-c453-df92530bb560"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== GPT-2 | Top-p ==\n",
            "Once upon a time, there was a kind of a lull in the world of war. In one country, they could not afford a small army to fight on their own. There was no way of telling which country was\n",
            "\n",
            "== GPT-2 | Top-p ==\n",
            "What is 2+2?\n",
            "\n",
            "The 2+2 concept is a basic way of thinking about numbers that may or may not be consistent with the laws of logic. The 2+2 concept is defined as follows:\n",
            "\n",
            "\n",
            "\n",
            "== GPT-2 | Top-p ==\n",
            "Suggest a party theme. I have a really large party. If you want to party with your friends or have a party you can make a party theme that will make your party even better.\n",
            "\n",
            "If you have a party\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "Top-p sampling (with p ≈ 0.9) usually produces smoother, more human-like text.  \n",
        "Because it samples from a variable-sized pool of likely tokens, it balances creativity and coherence better than greedy decoding.\n"
      ],
      "metadata": {
        "id": "JmCS9DjbalgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 – Try It Yourself\n",
        "\n",
        "Scroll to the list called `tests` above and modify it to include your own prompts.\n",
        "\n",
        "You can also experiment with these parameters:\n",
        "\n",
        "| Parameter | Description | Typical Range |\n",
        "|:--|:--|:--|\n",
        "| `strategy` | `\"greedy\"`, `\"top_k\"`, `\"top_p\"`, `\"beam\"` | — |\n",
        "| `temperature` | Controls randomness | 0.2 – 2.0 |\n",
        "| `top_k` | Number of tokens considered in top-k sampling | 10 – 100 |\n",
        "| `top_p` | Cumulative probability cutoff for top-p sampling | 0.8 – 0.95 |\n",
        "\n",
        "**Tip:**  \n",
        "Try generating the same prompt with both **greedy** and **top-p = 0.9** — notice how tone, vocabulary, and rhythm change with temperature adjustments.\n"
      ],
      "metadata": {
        "id": "EnTOqxiQap3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 – Completion vs. Instruction-Tuned LLMs\n",
        "\n",
        "So far, we’ve seen that we can use **GPT-2** to generate text continuations.  \n",
        "However, GPT-2 is a *completion model*: it simply continues the given text, without understanding it as a question or request.\n",
        "\n",
        "**Instruction-tuned LLMs** (like Qwen-Chat, ChatGPT, or Llama-2-Chat) go through an additional training stage called **post-training**.  \n",
        "This stage teaches them to interpret prompts as instructions rather than raw text.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Model Type | Behavior | Training Focus |\n",
        "|:--|:--|:--|\n",
        "| **Completion Model** (e.g., GPT-2) | Continues text in the same style | Predicts next tokens only |\n",
        "| **Instruction-Tuned Model** (e.g., Qwen-Chat) | Reads prompts as requests and responds helpfully | Fine-tuned with human feedback and dialogue data |\n",
        "\n",
        "---\n",
        "\n",
        "### Why Instruction-Tuned Models Feel “Smarter”\n",
        "\n",
        "Because of **post-training**, instruction-tuned models will:\n",
        "- Read the entire prompt as a *request*, not just as text to mimic  \n",
        "- Stay in dialogue mode, answering questions or following instructions  \n",
        "- Refuse unsafe or disallowed prompts  \n",
        "- Maintain a consistent persona (“Assistant”) rather than drifting into storytelling  \n"
      ],
      "metadata": {
        "id": "iDwUG1QLayIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 – Qwen1.5-Chat vs. GPT-2\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "MODELS = {\n",
        "    \"gpt2\": \"gpt2\",\n",
        "    \"qwen\": \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "}\n",
        "\n",
        "tokenizers, models = {}, {}\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load both GPT-2 and Qwen-Chat\n",
        "for key, mid in MODELS.items():\n",
        "    tok = AutoTokenizer.from_pretrained(mid)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(mid).eval().to(device)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    mdl.config.pad_token_id = tok.pad_token_id\n",
        "    tokenizers[key], models[key] = tok, mdl\n",
        "    print(f\"Loaded {mid} as {key}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gFfQ9Y8a01w",
        "outputId": "e0f2cffc-349d-404b-e155-5af54b3b8302"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded gpt2 as gpt2\n",
            "Loaded Qwen/Qwen1.5-1.8B-Chat as qwen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**  \n",
        "This downloads two small models:  \n",
        "- **GPT-2 (124 M parameters)** — a base *completion* model  \n",
        "- **Qwen-1.5-Chat (1.8 B parameters)** — an *instruction-tuned* chat model  \n",
        "\n",
        "The download may take a few minutes the first time; subsequent runs use cached models.\n"
      ],
      "metadata": {
        "id": "0DBwkfNma3Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare GPT-2 vs. Qwen-Chat on identical prompts\n",
        "\n",
        "tests = [\n",
        "    (\"Once upon a time\", \"greedy\"),\n",
        "    (\"What is 2+2?\", \"top_k\"),\n",
        "    (\"Suggest a party theme.\", \"top_p\")\n",
        "]\n",
        "\n",
        "for prompt, strategy in tests:\n",
        "    for key in [\"gpt2\", \"qwen\"]:\n",
        "        print(f\"\\n== {key.upper()} | {strategy} ==\")\n",
        "        print(generate(key, prompt, strategy, 80))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YteQzzRa6j5",
        "outputId": "67873761-7900-408b-a04b-571c43b14efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== GPT2 | greedy ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and\n",
            "\n",
            "== QWEN | greedy ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Comparison**\n",
        "\n",
        "1. **GPT-2 Output**  \n",
        "   - Treats the prompt as story text.  \n",
        "   - Produces narrative or associative continuations.  \n",
        "   - Doesn’t recognize commands or questions explicitly.\n",
        "\n",
        "2. **Qwen-Chat Output**  \n",
        "   - Interprets the prompt as a *task* or *question*.  \n",
        "   - Provides concise, direct answers.  \n",
        "   - Maintains an assistant-style tone.\n",
        "\n",
        "**In summary:**  \n",
        "Instruction-tuned models extend base LLMs by aligning them with *human intentions*, making them interactive, helpful, and safe for real-world use.\n"
      ],
      "metadata": {
        "id": "2VtymNfGa8Fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 – A Small LLM Playground (Optional)\n",
        "\n",
        "This optional section builds a **mini interactive playground** where you can:\n",
        "- Enter a text prompt  \n",
        "- Choose a model (GPT-2 or Qwen-Chat)  \n",
        "- Select a decoding strategy (greedy, top-k, or top-p)  \n",
        "- Adjust the temperature to control creativity  \n",
        "\n",
        "Press **Generate** to watch the model respond in real time.\n"
      ],
      "metadata": {
        "id": "FsWSG-q7bLVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "MODELS = {\n",
        "    \"gpt2\": \"gpt2\",\n",
        "    \"qwen\": \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "}\n",
        "\n",
        "tokenizers, models = {}, {}\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "for key, mid in MODELS.items():\n",
        "    tok = AutoTokenizer.from_pretrained(mid)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(mid).eval().to(device)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    mdl.config.pad_token_id = tok.pad_token_id\n",
        "    tokenizers[key], models[key] = tok, mdl\n",
        "    print(f\"Loaded {mid} as {key}\")\n",
        "\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Make sure models and tokenizers are loaded\n",
        "try:\n",
        "    tokenizers\n",
        "    models\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Please run the earlier setup cells that load the models before using the playground.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Text Generation Function\n",
        "# ---------------------------------------------------------\n",
        "def generate_playground(model_key, prompt, strategy=\"greedy\", temperature=1.0, max_new_tokens=100):\n",
        "    tok, mdl = tokenizers[model_key], models[model_key]\n",
        "    enc = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
        "    gen_args = dict(**enc, max_new_tokens=max_new_tokens, pad_token_id=tok.pad_token_id)\n",
        "\n",
        "    if strategy == \"greedy\":\n",
        "        gen_args[\"do_sample\"] = False\n",
        "    elif strategy == \"top_k\":\n",
        "        gen_args.update(dict(do_sample=True, top_k=50, temperature=temperature))\n",
        "    elif strategy == \"top_p\":\n",
        "        gen_args.update(dict(do_sample=True, top_p=0.9, temperature=temperature))\n",
        "    else:\n",
        "        raise ValueError(\"Unknown strategy\")\n",
        "\n",
        "    out = mdl.generate(**gen_args)\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Build Interactive UI\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Text box for the user prompt\n",
        "prompt_box = widgets.Textarea(\n",
        "    value=\"Tell me a fun fact about space.\",\n",
        "    placeholder=\"Type your prompt here\",\n",
        "    description=\"Prompt:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"120px\")\n",
        ")\n",
        "\n",
        "# Dropdown for model selection\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=[(\"GPT-2\", \"gpt2\"), (\"Qwen-1.5-Chat\", \"qwen\")],\n",
        "    value=\"gpt2\",\n",
        "    description=\"Model:\"\n",
        ")\n",
        "\n",
        "# Dropdown for decoding strategy\n",
        "strategy_dropdown = widgets.Dropdown(\n",
        "    options=[(\"Greedy\", \"greedy\"), (\"Top-k\", \"top_k\"), (\"Top-p\", \"top_p\")],\n",
        "    value=\"greedy\",\n",
        "    description=\"Strategy:\"\n",
        ")\n",
        "\n",
        "# Temperature slider\n",
        "temperature_slider = widgets.FloatSlider(\n",
        "    value=1.0,\n",
        "    min=0.1,\n",
        "    max=2.0,\n",
        "    step=0.1,\n",
        "    description=\"Temp:\"\n",
        ")\n",
        "\n",
        "# Generate button\n",
        "generate_button = widgets.Button(description=\"Generate\", button_style=\"primary\")\n",
        "\n",
        "# Output area\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Define button callback\n",
        "# ---------------------------------------------------------\n",
        "def on_generate(_):\n",
        "    output_area.clear_output()\n",
        "    with output_area:\n",
        "        try:\n",
        "            result = generate_playground(\n",
        "                model_dropdown.value,\n",
        "                prompt_box.value,\n",
        "                strategy_dropdown.value,\n",
        "                temperature_slider.value\n",
        "            )\n",
        "            display(Markdown(f\"**Output:**\\n\\n{result}\"))\n",
        "        except Exception as e:\n",
        "            print(\"Error:\", e)\n",
        "\n",
        "# Attach callback to button\n",
        "generate_button.on_click(on_generate)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Layout and Display\n",
        "# ---------------------------------------------------------\n",
        "ui = widgets.VBox([\n",
        "    prompt_box,\n",
        "    widgets.HBox([model_dropdown, strategy_dropdown, temperature_slider]),\n",
        "    generate_button,\n",
        "    output_area\n",
        "])\n",
        "\n",
        "display(ui)\n"
      ],
      "metadata": {
        "id": "4c9T2dxmbwP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "86uc1ZqAbR6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to Use**\n",
        "\n",
        "1. Type your own prompt in the text box.  \n",
        "2. Choose a model and decoding strategy.  \n",
        "3. Adjust the *temperature* (lower = precise / higher = creative).  \n",
        "4. Press **Generate** to see the model’s output below.  \n",
        "\n",
        "**Tip:**  \n",
        "Try the same prompt with both GPT-2 and Qwen-Chat — notice how GPT-2 continues text, while Qwen-Chat interprets your request and responds conversationally.\n"
      ],
      "metadata": {
        "id": "tb56harsbVze"
      }
    }
  ]
}